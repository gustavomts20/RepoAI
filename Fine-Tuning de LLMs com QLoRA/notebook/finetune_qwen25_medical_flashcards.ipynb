{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d8f18e",
   "metadata": {},
   "source": [
    "# Fine‑Tuning Qwen2.5 em flashcards médicos com QLoRA\n",
    "\n",
    "Este notebook demonstra como fazer o fine-tuning de um modelo Qwen2.5 em um conjunto de dados de flashcards médicos usando QLoRA. A abordagem é leve o suficiente para rodar em uma única GPU T4 no Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U pip\n",
    "\n",
    "!pip -q install \"numpy>=2,<2.1\"\n",
    "\n",
    "!pip -q install -U \\\n",
    "  \"transformers>=4.41,<5\" \\\n",
    "  \"datasets>=2.19.0\" \\\n",
    "  \"accelerate>=0.28.0\" \\\n",
    "  \"peft>=0.10.0\" \\\n",
    "  \"bitsandbytes>=0.43.0\" \\\n",
    "  \"trl>=0.9.6\" \\\n",
    "  \"evaluate>=0.4.1\" \\\n",
    "  \"fsspec==2025.3.0\" \\\n",
    "  \"gcsfs==2025.3.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812bf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(round(torch.cuda.get_device_properties(0).total_memory/1024/1024/1024,2))\n",
    "else:\n",
    "    print(\"CUDA indisponível\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6f4be",
   "metadata": {},
   "source": [
    "## Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ca2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "DATASET_NAME = \"flwrlabs/medical-meadow-medical-flashcards\"\n",
    "MAX_SEQ_LEN = 1024\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 1\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "ADAPTER_OUTPUT_DIR = \"outputs/adapter\"\n",
    "MERGED_OUTPUT_DIR = \"outputs/merged\"\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c74b6",
   "metadata": {},
   "source": [
    "## Carregar o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69588a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "split = dataset.train_test_split(test_size=0.05, seed=SEED)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73239500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fda02f",
   "metadata": {},
   "source": [
    "## Formatar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(example):\n",
    "    user_content = example[\"instruction\"] + \"\\n\" + example[\"input\"]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful medical assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_dataset = train_dataset.map(apply_template, remove_columns=train_dataset.column_names)\n",
    "eval_dataset = eval_dataset.map(apply_template, remove_columns=eval_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0474b",
   "metadata": {},
   "source": [
    "## Tokenizar os prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfe7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(sample):\n",
    "    result = tokenizer(sample[\"text\"], truncation=True, max_length=MAX_SEQ_LEN, padding=False)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=eval_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29702722",
   "metadata": {},
   "source": [
    "## Definir o colator de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def data_collator(features):\n",
    "    input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
    "    attention_mask = [torch.tensor(f[\"attention_mask\"], dtype=torch.long) for f in features]\n",
    "    labels = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in features]\n",
    "    batch = {}\n",
    "    batch[\"input_ids\"] = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    batch[\"attention_mask\"] = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    batch[\"labels\"] = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3130f3",
   "metadata": {},
   "source": [
    "## Carregar o modelo com QLoRA e aplicar adaptadores LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43164394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable = 0\n",
    "    total = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        total += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable += param.numel()\n",
    "    print(trainable)\n",
    "    print(total)\n",
    "    print(trainable/total)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9993d",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a0a09a",
   "metadata": {},
   "source": [
    "## Salvar o adaptador e o tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(ADAPTER_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(ADAPTER_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4964f5",
   "metadata": {},
   "source": [
    "## Opcional: mesclar o LoRA ao modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed73d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "try:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "    peft_model = PeftModel.from_pretrained(base_model, ADAPTER_OUTPUT_DIR)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4f90b",
   "metadata": {},
   "source": [
    "## Demonstração de inferência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cded9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_OUTPUT_DIR)\n",
    "model.eval()\n",
    "\n",
    "def chat(query):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful medical assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=1280, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "queries = [\n",
    "    \"A 67-year-old with atrial fibrillation is on warfarin and starts trimethoprim-sulfamethoxazole. What complication is most likely, and why?\",\n",
    "    \"A patient with chronic kidney disease has persistently high phosphate and low calcium. What happens to PTH over time, and what bone changes can result?\",\n",
    "    \"After 10 days of clindamycin, a patient develops watery diarrhea and abdominal cramping. What is the likely diagnosis and first-line treatment?\",\n",
    "    \"A newborn becomes lethargic with poor feeding and vomiting. Labs show ammonia is markedly elevated with normal glucose. Name a likely metabolic disorder category and an initial management step.\",\n",
    "    \"A 24-year-old has fever, dysuria, and flank pain with nausea. Urinalysis shows WBC casts. What diagnosis is most likely and what empiric antibiotics are commonly used?\"\n",
    "]\n",
    "for q in queries:\n",
    "    print(q)\n",
    "    print(chat(q))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
